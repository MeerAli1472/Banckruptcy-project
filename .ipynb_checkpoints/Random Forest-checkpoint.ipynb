{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26210f09",
   "metadata": {},
   "source": [
    "# 5.3. Ensemble Models: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf2473",
   "metadata": {},
   "source": [
    "So far in this project, we've learned how to retrieve and decompress data, and how to manage imbalanced data to build a decision-tree model.\n",
    "\n",
    "In this lesson, we're going to expand our decision tree model into an entire forest (an example of something called an **ensemble model**); learn how to use a **grid search** to tune hyperparameters; and create a function that loads data and a pre-trained model, and uses that model to generate a Series of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from IPython.display import VimeoVideo\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc55b0f9",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "As always, we'll begin by importing the dataset.\n",
    "\n",
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53027f",
   "metadata": {},
   "source": [
    "**Task 5.3.1:** Complete the `wrangle` function below using the code you developed in the  lesson 5.1. Then use it to import `poland-bankruptcy-data-2009.json.gz` into the DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b5b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(filename):\n",
    "    # Open compressed file, load into dictionary\n",
    "    with gzip.open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "\n",
    "    # Load dictionary into DataFrame, set index\n",
    "    df = pd.DataFrame().from_dict(data[\"data\"]).set_index(\"company_id\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd80df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle(\"data/poland-bankruptcy-data-2009.json.gz\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a844417",
   "metadata": {},
   "source": [
    "# split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8191f3",
   "metadata": {},
   "source": [
    "**Task 5.3.2:** Create your feature matrix `X` and target vector `y`. Your target is `\"bankrupt\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4220fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"bankrupt\"\n",
    "X =df.drop(columns=target)\n",
    "y = df[target]\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c3448",
   "metadata": {},
   "source": [
    "Since we're not working with time series data, we're going to randomly divide our dataset into training and test sets ‚Äî just like we did in project 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a94dbd",
   "metadata": {},
   "source": [
    "**Task 5.3.3:** Divide your data (`X` and `y`) into training and test sets using a randomized train-test split. Your test set should be 20% of your total data. And don't forget to set a `random_state` for reproducibility. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbbd8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d374c24",
   "metadata": {},
   "source": [
    "You might have noticed that we didn't create a validation set, even though we're planning on tuning our model's hyperparameters in this lesson. That's because we're going to use cross-validation, which we'll talk about more later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96bc52",
   "metadata": {},
   "source": [
    "# Resample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a098bc89",
   "metadata": {},
   "source": [
    "**Task 5.3.4:** Create a new feature matrix `X_train_over` and target vector `y_train_over` by performing random over-sampling on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ebf981",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_sampler = RandomOverSampler(random_state=42)\n",
    "X_train_over, y_train_over = over_sampler.fit_resample(X_train, y_train)\n",
    "print(\"X_train_over shape:\", X_train_over.shape)\n",
    "X_train_over.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8011e458",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "Now that we have our data set up the right way, we can build the model. üèó\n",
    "\n",
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a953efe",
   "metadata": {},
   "source": [
    "**Task 5.3.5:** Calculate the baseline accuracy score for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48434b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_baseline = y_train.value_counts(normalize = True).max()\n",
    "print(\"Baseline Accuracy:\", round(acc_baseline, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803954c6",
   "metadata": {},
   "source": [
    "# iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10aeac1",
   "metadata": {},
   "source": [
    "So far, we've built single models that predict a single outcome. That's definitely a useful way to predict the future, but what if the one model we built isn't the *right* one? If we could somehow use more than one model simultaneously, we'd have a more trustworthy prediction.\n",
    "\n",
    "**Ensemble models** work by building multiple models on random subsets of the same data, and then comparing their predictions to make a final prediction. Since we used a decision tree in the last lesson, we're going to create an ensemble of trees here. This type of model is called a **random forest**.\n",
    "\n",
    "We'll start by creating a pipeline to streamline our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90bb5bc",
   "metadata": {},
   "source": [
    "**Task 5.3.6:** Create a pipeline named `clf` (short for \"classifier\") that contains a `SimpleImputer` transformer and a `RandomForestClassifier` predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(SimpleImputer(), RandomForestClassifier(random_state=42))\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c9bee8",
   "metadata": {},
   "source": [
    "By default, the number of trees in our forest (n_estimators) is set to 100. That means when we train this classifier, we'll be fitting 100 trees. While it will take longer to train, it will hopefully lead to better performance.\n",
    "\n",
    "In order to get the best performance from our model, we need to tune its hyperparameter. But how can we do this if we haven't created a validation set? The answer is cross-validation. So, before we look at hyperparameters, let's see how cross-validation works with the classifier we just built."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a7eff",
   "metadata": {},
   "source": [
    "**Task 5.3.7:** Perform cross-validation with your classifier, using the over-sampled training data. We want five folds, so set `cv` to 5. We also want to speed up training, to set `n_jobs` to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc3f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc_scores = cross_val_score(clf, X_train_over, y_train_over, cv = 5, n_jobs = -1)\n",
    "print(cv_acc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0667ccb",
   "metadata": {},
   "source": [
    "That took kind of a long time, but we just trained 500 random forest classifiers (100 jobs x 5 folds). No wonder it takes so long!\n",
    "\n",
    "Pro tip: even though cross_val_score is useful for getting an idea of how cross-validation works, you'll rarely use it. Instead, most people include a cv argument when they do a hyperparameter search.\n",
    "\n",
    "Now that we have an idea of how cross-validation works, let's tune our model. The first step is creating a range of hyperparameters that we want to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d098bd79",
   "metadata": {},
   "source": [
    "**Task 5.3.8:** Create a dictionary with the range of hyperparameters that we want to evaluate for our classifier. \n",
    "\n",
    "1. For the `SimpleImputer`, try both the `\"mean\"` and `\"median\"` strategies. \n",
    "2. For the `RandomForestClassifier`, try `max_depth` settings between 10 and 50, by steps of 10. \n",
    "3. Also for the `RandomForestClassifier`, try `n_estimators` settings between 25 and 100 by steps of 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"simpleimputer_strategy\": [\"mean\", \"median\"],\n",
    "        \"randomforestclassifier_n_estimators\": range(25,100,25),\n",
    "        \"randomforestclassifier_max_depth\": range(10,50,10)\n",
    "}\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce38295",
   "metadata": {},
   "source": [
    "Now that we have our hyperparameter grid, let's incorporate it into a grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52a15c",
   "metadata": {},
   "source": [
    "**Task 5.3.9:** Create a `GridSearchCV` named `model` that includes your classifier and hyperparameter grid. Be sure to use the same arguments for `cv` and `n_jobs` that you used above, and set `verbose` to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab0a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GridSearchCV(\n",
    "    clf,\n",
    "    param_grid=params,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    "    \n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2119ed",
   "metadata": {},
   "source": [
    "**Task 5.3.10:** Fit `model` to the over-sampled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe559cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e179658",
   "metadata": {},
   "source": [
    "This will take some time to train, so let's take a moment to think about why. How many forests did we just test? 4 different `max_depth`s times 3 `n_estimator`s times 2 imputation strategies... that makes 24 forests. How many fits did we just do? 24 forests times 5 folds is 120. And remember that each forest is comprised of 25-75 trees, which works out to *at least* 3,000 trees. So it's computationally expensive! \n",
    "\n",
    "Okay, now that we've tested all those models, let's take a look at the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eee9159",
   "metadata": {},
   "source": [
    "**Task 5.3.11:** Extract the cross-validation results from `model` and load them into a DataFrame named `cv_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa147997",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(model.cv_results_)\n",
    "cv_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc377f99",
   "metadata": {},
   "source": [
    "In addition to the accuracy scores for all the different models we tried during our grid search, we can see how long it took each model to train. Let's take a closer look at how different hyperparameter settings affect training time. \n",
    "\n",
    "First, we'll look at `n_estimators`. Our grid search evaluated this hyperparameter for various `max_depth` settings, but let's only look at models where `max_depth` equals 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c549d",
   "metadata": {},
   "source": [
    "**Task 5.3.12:** Create a mask for `cv_results` for rows where `\"param_randomforestclassifier__max_depth\"` equals 10. Then plot `\"param_randomforestclassifier__n_estimators\"` on the x-axis and `\"mean_fit_time\"` on the y-axis. Don't forget to label your axes and include a title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cv_results[\"param_randomforestclassifier__max_depth\"]==10\n",
    "cv_results[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask\n",
    "mask = cv_results[\"param_randomforestclassifier__max_depth\"]==10\n",
    "# Plot fit time vs n_estimators\n",
    "plt.plot(\n",
    "    cv_results[mask][\"param_randomforestclassifier__n_estimators\"],\n",
    "    cv_results[mask][\"mean_fit_time\"]\n",
    ")\n",
    "# Label axes\n",
    "plt.xlabel(\"Number of Estimators\")\n",
    "plt.ylabel(\"Mean Fit Time [seconds]\")\n",
    "plt.title(\"Training Time vs Estimators (max_depth=10)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c8252",
   "metadata": {},
   "source": [
    "Next, we'll look at max_depth. Here, we'll also limit our data to rows where n_estimators equals 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3928134",
   "metadata": {},
   "source": [
    "**Task 5.3.13:** Create a mask for `cv_results` for rows where `\"param_randomforestclassifier__n_estimators\"` equals 25. Then plot `\"param_randomforestclassifier__max_depth\"` on the x-axis and `\"mean_fit_time\"` on the y-axis. Don't forget to label your axes and include a title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f204ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cv_results[\"param_randomforestclassifier__n_estimators\"]==25\n",
    "# Plot fit time vs n_estimators\n",
    "plt.plot(\n",
    "    cv_results[mask][\"param_randomforestclassifier__max_depth\"],\n",
    "    cv_results[mask][\"mean_fit_time\"]\n",
    ")\n",
    "# Label axes\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"Mean Fit Time [seconds]\")\n",
    "plt.title(\"Training Time vs Max Depth (n_estimators=25)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f74de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[mask][[\"mean_fit_time\",\"param_randomforestclassifier__max_depth\",\"param_simpleimputer__strategy\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae0484",
   "metadata": {},
   "source": [
    "There's a general upwards trend, but we see a lot of up-and-down here. That's because for each max depth, grid search tries two different imputation strategies: mean and median. Median is a lot faster to calculate, so that speeds up training time.\n",
    "\n",
    "Finally, let's look at the hyperparameters that led to the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fe414",
   "metadata": {},
   "source": [
    "**Task 5.3.14:** Extract the best hyperparameters from `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best hyperparameters\n",
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d41255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#also you can see model prediction\n",
    "model.predict(X_train_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf99f359",
   "metadata": {},
   "source": [
    "Note that we don't need to build and train a new model with these settings. Now that the grid search is complete, when we use model.predict(), it will serve up predictions using the best model ‚Äî something that we'll do at the end of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3adbada",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "All right: The moment of truth. Let's see how our model performs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d3b700",
   "metadata": {},
   "source": [
    "**Task 5.3.15:** Calculate the training and test accuracy scores for `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc223cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = model.score(X_train_over, y_train_over)\n",
    "acc_test = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Accuracy:\",acc_train)\n",
    "print(\"Test Accuracy:\", round(acc_test, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb65d2b1",
   "metadata": {},
   "source": [
    "We beat the baseline! Just barely, but we beat it.\n",
    "\n",
    "Next, we're going to use a confusion matrix to see how our model performs. To better understand the values we'll see in the matrix, let's first count how many observations in our test set belong to the positive and negative classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99410e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6074f1",
   "metadata": {},
   "source": [
    "**Task 5.3.16:** Plot a confusion matrix that shows how your best model performs on your test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(model,X_test,y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de55048",
   "metadata": {},
   "source": [
    "Notice the relationship between the numbers in this matrix with the count you did the previous task. If you sum the values in the bottom row, you get the total number of positive observations in y_test ( 72+11=83\n",
    " ). And the top row sum to the number of negative observations ( 1903+10=1913\n",
    " )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4bfe56",
   "metadata": {},
   "source": [
    "# Communicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722e0d1",
   "metadata": {},
   "source": [
    "**Task 5.3.17:** Create a horizontal bar chart with the 10 most important features for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e246d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names from training data\n",
    "features = X_train_over.columns\n",
    "# Extract importances from model\n",
    "importances =  model.best_estimator_.named_steps[\"randomforestclassifier\"].feature_importances_\n",
    "# Create a series with feature names and importances\n",
    "feat_imp =  pd.Series(importances, index =features).sort_values()\n",
    "# Plot 10 most important features\n",
    "feat_imp.tail(10).plot(kind = \"barh\")\n",
    "plt.xlabel(\"Gini Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946a739",
   "metadata": {},
   "source": [
    "**Task 5.3.18:** Using a context manager, save your best-performing model to a a file named `\"model-5-3.pkl\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e56f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "with open(\"model-5-3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfbbf45",
   "metadata": {},
   "source": [
    "**Task 5.3.19:** Create a function `make_predictions`. It should take two arguments: the path of a JSON file that contains test data and the path of a serialized model. The function should load and clean the data using the `wrangle` function you created, load the model, generate an array of predictions, and convert that array into a Series. (The Series should have the name `\"bankrupt\"` and the same index labels as the test data.) Finally, the function should return its predictions as a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(data_filepath, model_filepath):\n",
    "    # Wrangle JSON file\n",
    "    X_test = wrangle(data_filepath)\n",
    "    # Load model\n",
    "    with open(model_filepath, \"rb\") as f:\n",
    "        model=pickle.load(f)\n",
    "        \n",
    "    # Generate predictions\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    # Put predictions into Series with name \"bankrupt\", and same index as X_test\n",
    "    y_test_pred = pd.Series(y_test_pred, index=X_test.index , name=\"bankrupt\")\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17871e0f",
   "metadata": {},
   "source": [
    "**Task 5.3.20:** Use the code below to check your `make_predictions` function. Once you're satisfied with the result, submit it to the grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86129f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = make_predictions(\n",
    "    data_filepath=\"data/poland-bankruptcy-data-2009-mvp-features.json.gz\",\n",
    "    model_filepath=\"model-5-3.pkl\",\n",
    ")\n",
    "\n",
    "print(\"predictions shape:\", y_test_pred.shape)\n",
    "y_test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422f802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
