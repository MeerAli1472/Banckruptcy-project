{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094a2c42",
   "metadata": {},
   "source": [
    "# 5.2. Imbalanced Data\n",
    "\n",
    "In the last lesson, we prepared the data.\n",
    "\n",
    "In this lesson, we're going to explore some of the features of the dataset, use visualizations to help us understand those features, and develop a model that solves the problem of imbalanced data by under- and over-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd2b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8454805f",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "## Import\n",
    "As always, we need to begin by bringing our data into the project, and the function we developed in the previous module is exactly what we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715231d6",
   "metadata": {},
   "source": [
    "**Task 5.2.1:** Complete the `wrangle` function below using the code you developed in the last lesson. Then use it to import `poland-bankruptcy-data-2009.json.gz` into the DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4117c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(filename):\n",
    "    \n",
    "    # Open compressed file, load into dictionary\n",
    "    with gzip.open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "\n",
    "    # Load dictionary into DataFrame, set index\n",
    "    df = pd.DataFrame().from_dict(data[\"data\"]).set_index(\"company_id\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85dcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle(\"data/poland-bankruptcy-data-2009.json.gz\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b9c3ee",
   "metadata": {},
   "source": [
    "# Explore\n",
    "Let's take a moment to refresh our memory on what's in this dataset. In the last lesson, we noticed that the data was stored in a JSON file (similar to a Python dictionary), and we explored the key-value pairs. This time, we're going to look at what the values in those pairs actually are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58fcbd2",
   "metadata": {},
   "source": [
    "**Task 5.2.2:** Use the `info` method to explore `df`. What type of features does this dataset have? Which column is the target? Are there columns will missing values that we'll need to address?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect DataFrame\n",
    "df.shape\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a2df7",
   "metadata": {},
   "source": [
    "That's solid information. We know all our features are numerical and that we have missing data. But, as always, it's a good idea to do some visualizations to see if there are any interesting trends or ideas we should keep in mind while we work. First, let's take a look at how many firms are bankrupt, and how many are not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528b5fa",
   "metadata": {},
   "source": [
    "**Task 5.2.3:** Create a bar chart of the value counts for the `\"bankrupt\"` column. You want to calculate the relative frequencies of the classes, not the raw count, so be sure to set the `normalize` argument to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4451a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class balance\n",
    "df[\"bankrupt\"].value_counts(normalize = True).plot(\n",
    "    kind = \"bar\",\n",
    "    xlabel = \"Bankrupt\",\n",
    "    ylabel = \"Frequency\",\n",
    "    title = \"class balance\"\n",
    "    \n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf6942",
   "metadata": {},
   "source": [
    "That's good news for Poland's economy! Since it looks like most of the companies in our dataset are doing all right for themselves, let's drill down a little farther. However, it also shows us that we have an imbalanced dataset, where our majority class is far bigger than our minority class.\n",
    "\n",
    "In the last lesson, we saw that there were 64 features of each company, each of which had some kind of numerical value. It might be useful to understand where the values for one of these features cluster, so let's make a boxplot to see how the values in \"feat_27\" are distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa2dfd9",
   "metadata": {},
   "source": [
    "**Task 5.2.4:** Use seaborn to create a boxplot that shows the distributions of the `\"feat_27\"` column for both groups in the `\"bankrupt\"` column. Remember to label your axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37bb90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot\n",
    "sns.boxplot(x=df[\"bankrupt\"], y=df[\"feat_27\"])\n",
    "plt.xlabel(\"Bankrupt\")\n",
    "plt.ylabel(\"POA / financial expenses\")\n",
    "plt.title(\"Distribution of Profit/Expenses Ratio, by Class\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae736fff",
   "metadata": {},
   "source": [
    "Why does this look so funny? Remember that boxplots exist to help us see the quartiles in a dataset, and this one doesn't really do that. Let's check the distribution of \"feat_27\"to see if we can figure out what's going on here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae90f0e",
   "metadata": {},
   "source": [
    "**Task 5.2.5:** Use the `describe` method on the column for `\"feat_27\"`. What can you tell about the distribution of the data based on the mean and median?m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd443e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for `feat_27`\n",
    "df[\"feat_27\"].describe().apply(\"{0:,.0f}\".format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956edd8",
   "metadata": {},
   "source": [
    "Hmmm. Note that the median is around 1, but the mean is over 1000. That suggests that this feature is skewed to the right. Let's make a histogram to see what the distribution actually looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a0503",
   "metadata": {},
   "source": [
    "**Task 5.2.6:** Create a histogram of `\"feat_27\"`. Make sure to label x-axis `\"POA / financial expenses\"`, the y-axis `\"Count\"`, and use the title `\"Distribution of Profit/Expenses Ratio\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40766ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of `feat_27`\n",
    "df[\"feat_27\"].hist()\n",
    "plt.xlabel(\"POA / financial expenses\")\n",
    "plt.ylabel(\"Count\"),\n",
    "plt.title(\"Distribution of Profit/Expenses Ratio\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b68412",
   "metadata": {},
   "source": [
    "Aha! We saw it in the numbers and now we see it in the histogram. The data is very skewed. So, in order to create a helpful boxplot, we need to trim the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3359ea36",
   "metadata": {},
   "source": [
    "**Task 5.2.7:** Recreate the boxplot that you made above, this time only using the values for `\"feat_27\"` that fall between the `0.1` and `0.9` quantiles for the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd40357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clipped boxplot\n",
    "q1, q9 = df[\"feat_27\"].quantile([0.1,0.9])\n",
    "mask = df[\"feat_27\"].between(q1,q9)\n",
    "sns.boxplot(x=\"bankrupt\",y=\"feat_27\", data=df[mask])\n",
    "plt.xlabel(\"Bankrupt\")\n",
    "plt.ylabel(\"POA / financial expenses\")\n",
    "plt.title(\"Distribution of Profit/Expenses Ratio, by Bankruptcy Status\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e3411",
   "metadata": {},
   "source": [
    "That makes a lot more sense. Let's take a look at some of the other features in the dataset to see what else is out there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3dbc64",
   "metadata": {},
   "source": [
    "**Task 5.2.8:** Repeat the exploration you just did for `\"feat_27\"` on two other features in the dataset. Do they show the same skewed distribution? Are there large differences between bankrupt and solvent companies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c497247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b783671",
   "metadata": {},
   "source": [
    "Looking at other features, we can see that they're skewed, too. This will be important to keep in mind when we decide what type of model we want to use.\n",
    "\n",
    "Another important consideration for model selection is whether there are any issues with multicollinearity in our model. Let's check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce705d09",
   "metadata": {},
   "source": [
    "**Task 5.2.9:** Plot a correlation heatmap of features in `df`. Since `\"bankrupt\"` will be your target, you don't need to include it in your heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd63ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.drop(columns = \"bankrupt\").corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83be5a83",
   "metadata": {},
   "source": [
    "​\n",
    "So what did we learn from this EDA? First, our data is imbalanced. This is something we need to address in our data preparation. Second, many of our features have missing values that we'll need to impute. And since the features are highly skewed, the best imputation strategy is likely median, not mean. Finally, we have autocorrelation issues, which means that we should steer clear of linear models, and try a tree-based model instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c484e",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c283592",
   "metadata": {},
   "source": [
    "So let's start building that model. If you need a refresher on how and why we split data in these situations, take a look back at the Time Series module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3589c7f4",
   "metadata": {},
   "source": [
    "**Task 5.2.10:** Create your feature matrix `X` and target vector `y`. Your target is `\"bankrupt\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d18eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"bankrupt\"\n",
    "X =df.drop(columns=target)\n",
    "y = df[target]\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d5e0f",
   "metadata": {},
   "source": [
    "In order to make sure that our model can generalize, we need to put aside a test set that we'll use to evaluate our model once it's trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873dfc42",
   "metadata": {},
   "source": [
    "**Task 5.2.11:** Divide your data (`X` and `y`) into training and test sets using a randomized train-test split. Your validation set should be 20% of your total data. And don't forget to set a `random_state` for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0918cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a78ad",
   "metadata": {},
   "source": [
    "Note that if we wanted to tune any hyperparameters for our model, we'd do another split here, further dividing the training set into training and validation sets. However, we're going to leave hyperparameters for the next lesson, so no need to do the extra split now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8da687",
   "metadata": {},
   "source": [
    "# Resample\n",
    "Now that we've split our data into training and validation sets, we can address the class imbalance we saw during our EDA. One strategy is to resample the training data. (This will be different than the resampling we did with time series data in Project 3.) There are many to do this, so let's start with under-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8da3d0",
   "metadata": {},
   "source": [
    "**Task 5.2.12:** Create a new feature matrix `X_train_under` and target vector `y_train_under` by performing random under-sampling on your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd59c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_train_under, y_train_under = under_sampler.fit_resample(X_train, y_train)\n",
    "print(X_train_under.shape)\n",
    "X_train_under.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8795c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_under.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b44b6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <b>Note:</b> Depending on the random state you set above, you may get a different  shape for <code>X_train_under</code>. Don't worry, it's normal!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677ce0e",
   "metadata": {},
   "source": [
    "And then we'll over-sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88262b3",
   "metadata": {},
   "source": [
    "**Task 5.2.13:** Create a new feature matrix `X_train_over` and target vector `y_train_over` by performing random over-sampling on your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63f861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_sampler = RandomOverSampler(random_state=42)\n",
    "X_train_over, y_train_over = over_sampler.fit_resample(X_train,y_train)\n",
    "print(X_train_over.shape)\n",
    "X_train_over.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d1f42",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "## Baseline\n",
    "As always, we need to establish the baseline for our model. Since this is a classification problem, we'll use accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7530b7b2",
   "metadata": {},
   "source": [
    "**Task 5.2.14:** Calculate the baseline accuracy score for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ac800",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_baseline = y_train.value_counts(normalize = True).max()\n",
    "print(\"Baseline Accuracy:\", round(acc_baseline, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8bb7dc",
   "metadata": {},
   "source": [
    "Note here that, because our classes are imbalanced, the baseline accuracy is very high. We should keep this in mind because, even if our trained model gets a high validation accuracy score, that doesn't mean it's actually good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7862e8",
   "metadata": {},
   "source": [
    "# Iterate\n",
    "Now that we have a baseline, let's build a model to see if we can beat it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e892c",
   "metadata": {},
   "source": [
    "**Task 5.2.15:** Create three identical models: `model_reg`, `model_under` and `model_over`. All of them should use a `SimpleImputer` followed by a `DecisionTreeClassifier`. Train `model_reg` using the unaltered training data. For `model_under`, use the undersampled data. For `model_over`, use the oversampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e476434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on `X_train`, `y_train`\n",
    "model_reg = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"), DecisionTreeClassifier(random_state=42)\n",
    ")\n",
    "model_reg.fit(X_train, y_train)\n",
    "\n",
    "# Fit on `X_train_under`, `y_train_under`\n",
    "model_under = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"), DecisionTreeClassifier(random_state=42)\n",
    ")\n",
    "model_under.fit(X_train_under, y_train_under)\n",
    "\n",
    "# Fit on `X_train_over`, `y_train_over`\n",
    "model_over = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"), DecisionTreeClassifier(random_state=42)\n",
    ")\n",
    "model_over.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20605a1",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "How did we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f242274",
   "metadata": {},
   "source": [
    "**Task 5.2.16:** Calculate training and test accuracy for your three models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60cfa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [model_reg, model_under, model_over]:\n",
    "    acc_train = m.score(X_train, y_train)\n",
    "    acc_test = m.score(X_test, y_test)\n",
    "\n",
    "    print(\"Training Accuracy:\", round(acc_train, 4))\n",
    "    print(\"Test Accuracy:\", round(acc_test, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c059822",
   "metadata": {},
   "source": [
    "As we mentioned earlier, \"good\" accuracy scores don't tell us much about the model's performance when dealing with imbalanced data. So instead of looking at what the model got right or wrong, let's see how its predictions differ for the two classes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4e91a",
   "metadata": {},
   "source": [
    "**Task 5.2.17:** Plot a confusion matrix that shows how your best model performs on your validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f79c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(model_reg,X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3be702",
   "metadata": {},
   "source": [
    "In this lesson, we didn't do any hyperparameter tuning, but it will be helpful in the next lesson to know what the depth of the tree model_over."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ed761",
   "metadata": {},
   "source": [
    "**Task 5.2.18:** Determine the depth of the decision tree in `model_over`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6916d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = model_over.named_steps[\"decisiontreeclassifier\"].get_depth()\n",
    "print(depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38f0f0",
   "metadata": {},
   "source": [
    "# Communicate\n",
    "Now that we have a reasonable model, let's graph the importance of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25bef7f",
   "metadata": {},
   "source": [
    "**Task 5.2.19:** Create a horizontal bar chart with the 15 most important features for `model_over`. Be sure to label your x-axis `\"Gini Importance\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709074d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get importances\n",
    "importances = model_over.named_steps[\"decisiontreeclassifier\"].feature_importances_\n",
    "\n",
    "# Put importances into a Series\n",
    "feat_imp = pd.Series(importances, index = X_train_over.columns).sort_values()\n",
    "\n",
    "# Plot series\n",
    "feat_imp.tail(15).plot(kind = \"barh\")\n",
    "plt.xlabel(\"Gini Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"model_over Feature Importance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4550dbbb",
   "metadata": {},
   "source": [
    "There's our old friend \"feat_27\" near the top, along with features 34 and 26. It's time to share our findings.\n",
    "\n",
    "Sometimes communication means sharing a visualization. Other times, it means sharing the actual model you've made so that colleagues can use it on new data or deploy your model into production. First step towards production: saving your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a052b384",
   "metadata": {},
   "source": [
    "**Task 5.2.20:** Using a context manager, save your best-performing model to a a file named `\"model-5-2.pkl\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model as `\"model-5-2.pkl\"`\n",
    "with open(\"model-5-2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_over, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad2d0e",
   "metadata": {},
   "source": [
    "**Task 5.2.21:** Make sure you've saved your model correctly by loading `\"model-5-2.pkl\"` and assigning to the variable `loaded_model`. Once you're satisfied with the result, run the last cell to submit your model to the grader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931a4b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `\"model-5-2.pkl\"`\n",
    "with open(\"model-5-2.pkl\", \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d185864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
